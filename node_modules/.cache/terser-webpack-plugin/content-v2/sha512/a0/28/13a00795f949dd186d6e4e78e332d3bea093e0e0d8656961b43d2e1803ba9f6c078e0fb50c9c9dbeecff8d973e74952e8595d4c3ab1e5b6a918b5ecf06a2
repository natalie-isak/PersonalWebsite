{"map":"{\"version\":3,\"sources\":[\"component---src-pages-blog-1-js-8081701f0cd4c4404acb.js\"],\"names\":[\"window\",\"push\",\"1DgH\",\"module\",\"__webpack_exports__\",\"__webpack_require__\",\"r\",\"d\",\"pageQuery\",\"react__WEBPACK_IMPORTED_MODULE_0__\",\"react__WEBPACK_IMPORTED_MODULE_0___default\",\"n\",\"react_helmet__WEBPACK_IMPORTED_MODULE_1__\",\"styled_components__WEBPACK_IMPORTED_MODULE_2__\",\"_config__WEBPACK_IMPORTED_MODULE_3__\",\"_utils_sr__WEBPACK_IMPORTED_MODULE_4__\",\"_components__WEBPACK_IMPORTED_MODULE_5__\",\"div\",\"withConfig\",\"displayName\",\"componentId\",\"_ref\",\"theme\",\"mixins\",\"flexCenter\",\"_ref2\",\"location\",\"revealTitle\",\"data\",\"allMarkdownRemark\",\"edges\",\"Object\",\"revealTable\",\"revealProjects\",\"reveal\",\"current\",\"forEach\",\"ref\",\"i\",\"a\",\"createElement\",\"title\",\"className\"],\"mappings\":\"CAACA,OAAqB,aAAIA,OAAqB,cAAK,IAAIC,KAAK,CAAC,CAAC,GAAG,CAE5DC,OACA,SAAUC,EAAQC,EAAqBC,GAE7C,aACAA,EAAoBC,EAAEF,GACSC,EAAoBE,EAAEH,EAAqB,aAAa,WAAa,OAAOI,KACtF,IAAIC,EAAqCJ,EAAoB,QACzDK,EAA0DL,EAAoBM,EAAEF,GAChFG,EAA4CP,EAAoB,QAChEQ,EAAiDR,EAAoB,QACrES,EAAuCT,EAAoB,QAE3DU,EAAyCV,EAAoB,QAC7DW,EAA2CX,EAAoB,QACdA,EAAoB,QACrEQ,EAAgE,EAAEI,IAAIC,WAAW,CAACC,YAAY,8BAA8BC,YAAY,gBAAxIP,CAAyJ,CAAC,8nCAA8nC,iDAAgD,SAASQ,GAA2B,OAAXA,EAAKC,MAAmBC,OAAOC,cAAk5NpB,EAA6B,QAAj5N,SAAqBqB,GAAO,IAAIC,EAASD,EAAMC,SAAuEC,GAAzDF,EAAMG,KAAuBC,kBAAkBC,MAAsBC,OAAOtB,EAA2C,OAAlDsB,CAAqD,OAAUC,EAAYD,OAAOtB,EAA2C,OAAlDsB,CAAqD,MAAUE,EAAeF,OAAOtB,EAA2C,OAAlDsB,CAAqD,IAAkjB,OAA9iBA,OAAOtB,EAA8C,UAArDsB,EAAwD,WAAWhB,EAAwD,EAAEmB,OAAOP,EAAYQ,QAAQJ,OAAOjB,EAA+C,SAAtDiB,IAA4DhB,EAAwD,EAAEmB,OAAOF,EAAYG,QAAQJ,OAAOjB,EAA+C,SAAtDiB,CAAyD,IAAI,IAAIE,EAAeE,QAAQC,SAAQ,SAASC,EAAIC,GAAG,OAAOvB,EAAwD,EAAEmB,OAAOG,EAAIN,OAAOjB,EAA+C,SAAtDiB,CAA2D,GAAFO,SAAY,IAAuB5B,EAA2C6B,EAAEC,cAAcxB,EAAyD,EAAE,CAACU,SAASA,GAAuBhB,EAA2C6B,EAAEC,cAAc5B,EAA0D,EAAE,CAAC6B,MAAM,6BAA0C/B,EAA2C6B,EAAEC,cAAc,OAAO,KAAkB9B,EAA2C6B,EAAEC,cAAc,SAAS,CAACH,IAAIV,GAA0BjB,EAA2C6B,EAAEC,cAAc,KAAK,CAACE,UAAU,eAAe,mBAAgChC,EAA2C6B,EAAEC,cAAc,IAAI,CAACE,UAAU,YAAY,gDAA6DhC,EAA2C6B,EAAEC,cAAc,OAAO,KAAkB9B,EAA2C6B,EAAEC,cAAc,IAAI,KAAK,sLAA6M9B,EAA2C6B,EAAEC,cAAc,KAAK,MAAM,wsJAAg2J,IAAIhC,EAAU\"}","code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{\"1DgH\":function(e,t,i){\"use strict\";i.r(t),i.d(t,\"pageQuery\",(function(){return d}));var a=i(\"q1tI\"),n=i.n(a),o=i(\"qhky\"),r=i(\"vOnD\"),s=i(\"20nU\"),l=i(\"nLfd\"),h=i(\"Kvkj\");i(\"g67X\"),r.d.div.withConfig({displayName:\"blog1__StyledTableContainer\",componentId:\"sc-17uj44c-0\"})([\"margin:100px -20px;@media (max-width:768px){margin:50px -10px;}table{width:100%;border-collapse:collapse;.hide-on-mobile{@media (max-width:768px){display:none;}}tbody tr{&:hover,&:focus{background-color:var(--light-navy);}}th,td{padding:10px;text-align:left;&:first-child{padding-left:20px;@media (max-width:768px){padding-left:10px;}}&:last-child{padding-right:20px;@media (max-width:768px){padding-right:10px;}}svg{width:20px;height:20px;}}tr{cursor:default;td:first-child{border-top-left-radius:var(--border-radius);border-bottom-left-radius:var(--border-radius);}td:last-child{border-top-right-radius:var(--border-radius);border-bottom-right-radius:var(--border-radius);}}td{&.year{padding-right:20px;@media (max-width:768px){padding-right:10px;font-size:var(--fz-sm);}}&.title{padding-top:15px;padding-right:20px;color:var(--lightest-slate);font-size:var(--fz-xl);font-weight:600;line-height:1.25;}&.company{font-size:var(--fz-lg);white-space:nowrap;}&.tech{font-size:var(--fz-xxs);font-family:var(--font-mono);line-height:1.5;.separator{margin:0 5px;}span{display:inline-block;}}&.links{min-width:100px;div{display:flex;align-items:center;a{\",\";flex-shrink:0;}a + a{margin-left:10px;}}}}}\"],(function(e){return e.theme.mixins.flexCenter}));t.default=function(e){var t=e.location,i=(e.data.allMarkdownRemark.edges,Object(a.useRef)(null)),r=Object(a.useRef)(null),d=Object(a.useRef)([]);return Object(a.useEffect)((function(){l.a.reveal(i.current,Object(s.srConfig)()),l.a.reveal(r.current,Object(s.srConfig)(200,0)),d.current.forEach((function(e,t){return l.a.reveal(e,Object(s.srConfig)(10*t))}))}),[]),n.a.createElement(h.i,{location:t},n.a.createElement(o.a,{title:\"The Self Driving Trolley\"}),n.a.createElement(\"main\",null,n.a.createElement(\"header\",{ref:i},n.a.createElement(\"h1\",{className:\"big-heading\"},\"by Natalie Isak\"),n.a.createElement(\"p\",{className:\"subtitle\"},\"Published Thursday, August 15, 2019 7:06 PM\")),n.a.createElement(\"body\",null,n.a.createElement(\"p\",null,\" The “trolley problem” is an age-old philosophical conundrum that encapsulates the core of philosophical debate: is it more moral to do a bad thing or to let a worse thing happen?\",n.a.createElement(\"br\",null),\"Pippa Foot first proposed this thought experiment in 1967, and at the time it was just that: a thought experiment. Today, however, this situation is far from hypothetical. The traditional “trolley problem” begins like so: a person is driving a trolley on a track with five men working ahead, and one man working on the track to the right. The driver can either continue driving and let the five workers in the way die, or s/he can shift the train’s direction to the right and kill only one worker. Overall, this predicament is meant to showcase the core difference between utilitarian and deontological philosophies. In this situation, a deontologist would argue that one ought to let the five people die rather than kill one person. Deontologists believe that the morality of an action can be determined by analyzing the intention, rather than the consequence, of the action in accordance with a set of rules. These “rules” were proposed by Immanuel Kant and are traditionally known as the Categorical Imperative. The second principle of such imperative can be applied to the “trolley problem” since the driver would be using the one person as a means to an end. In other words, even if the end state of the action is favorable, it cannot be justified if it requires compromising the autonomy -- or freedom -- of another. On the other hand, a utilitarian would argue that the agent should always choose to kill one person die to save five. Utilitarianism is the philosophical belief that a moral action must maximize utility, or in other words, maximize pleasure. This theory only accounts for the consequences of actions, and therefore a utilitarian would disregard the distinction between positive and negative duties since these duties are only important when analyzing the intent of an agent. Only the end-result of the action -- one dead versus five dead -- would be morally relevant. Thus, a utilitarian would choose to turn the trolly right in order to maximize lives. Utilitarians believe this would be the morally correct course of action since the actor is maximizing life, and therefore happiness, for the greatest number of people. Today, if you replace “trolley” with “self-driving car,” you have one of the largest moral dilemmas facing companies such as GM and Ford. In the future, it will be programmers, not drivers, making life-or-death decisions. For example, if the brakes fail, should the car continue driving and hit five pedestrians? Or should it swerve, thereby killing the passenger? For an interesting simulation demonstrating the intricate situations engineers must consider, click here. I believe that given the unreliability and novelty of self-driving cars, one must refrain from purely deontological or consequentialist logic and always air on the side of killing the passenger, or passengers. The technology for self-driving cars is relatively new, and therefore it will take years before self-driving cars gain the full trust of the public. Given the controversial nature of the “trolley problem” and of self-driving cars, it would be wrong to impose (relatively) arbitrary moral decisions on the general population. Pedestrians, for instance, never consented to having self-driving cars. Thus, it would be immoral to have them pay the fatal consequences of self-driving cars. However, the people inside the self-driving cars did consent to the dangers of riding in a self-driving car, and therefore, it would be more acceptable to have them suffer the consequences. This view is largely in accordance with the belief that negative duties outweigh positive duties. Meaning, self-driving cars have a larger duty not to interfere with the lives of others (the pedestrians) than to provide aid. Given a utilitarian situation, even if it would be possible to swerve to save five passengers and kill one pedestrian, it would be immoral to do so given the negative duty not to harm the pedestrian. Conversely, in a deontological situation, if the car had the option of steering to save five passengers or staying on track to kill one pedestrian, a deontologist would argue it is best to stay on track and kill one pedestrian. However, in the context of self-driving cars, it would be morally preferable to swerve the car and kill the five passengers to avoid killing the uninvolved pedestrian. In conclusion, the “trolley problem” is an interesting philosophical thought experiment with infinite implications on the self-driving car business. Despite the allure of traditional approaches, however, it is ultimately infeasible to approach this issue from a purely deontological or utilitarian perspective. Until companies gain the trust and respect of the public, the self-driving car business ought to continue to have customers themselves bear the potential consequences.\"))))};var d=\"296413314\"}}]);","extractedComments":[]}